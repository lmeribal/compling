{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07bc76a",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "\n",
    "Опеределите части речи слов в этом тексте, используя пайморфи. В форму запишите слова, которым приписан тэг VERB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a246979",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Зонды будут запущены в космос в 2029 году и отправятся ко второй точке Лагранжа в системе Солнце\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec05223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "будут\n",
      "отправятся\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "for word in text.split():\n",
    "    parsed = morph.parse(word)[0].tag.POS\n",
    "    if parsed == 'VERB':\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beed7ea",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb5a6f",
   "metadata": {},
   "source": [
    "На данных предложениях обучите TFIDF векторайзер из sklearn (с дефолтными параметрами). Найдите слово с самым высоким коэффициентом tfidf в первом тексте. Вставьте его в форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b04a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(\n",
    "    'https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/anna_karenina.txt'\n",
    ")\n",
    "\n",
    "# работайте с этими предложениями\n",
    "sentences = r.text.split('\\n')\n",
    "sentences = [sent for sent in sentences if len(sent) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29f09ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer().fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42c0193e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1962"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(vec.transform([sentences[0]]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13dea779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'буржуазного'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()[1962]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb2e707",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c6808",
   "metadata": {},
   "source": [
    "Токенизируйте текст Анны Карениной с помощью библиотеки раздел, привидите все к нижнему регистру. Рассчитайте статистики биграммов и униграммов. Найдите наиболее вероятное продолжение \"красный ...\". Вставьте наиболее веротяное следующее слово в форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b0a586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import razdel\n",
    "from collections import Counter\n",
    "from gensim.models.phrases import Phrases\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "unigrams_bag = Counter()\n",
    "word_lst = []\n",
    "for sent in sentences:\n",
    "    for word in razdel.tokenize(sent):\n",
    "        unigrams_bag.update([word.text.lower()])\n",
    "        \n",
    "cnt_words = sum(list(unigrams_bag.values()))\n",
    "unigram_probas = Counter({word:c / cnt_words for word, c in unigrams_bag.items()})\n",
    "\n",
    "bigrams_bag = Counter()\n",
    "for sent in sentences:\n",
    "    sent_tokenized = [el.text.lower() for el in razdel.tokenize(sent)]\n",
    "    bigrams = ngrammer(sent_tokenized)\n",
    "    bigrams_bag.update(bigrams)\n",
    "    \n",
    "cnt_bigrams = sum(list(bigrams_bag.values()))\n",
    "bigram_probas = Counter({word:c / cnt_bigrams for word, c in bigrams_bag.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c7abd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((len(unigrams_bag), len(unigrams_bag)))\n",
    "id2word = list(unigrams_bag.keys())\n",
    "word2id = {word:i for i, word in enumerate(id2word)}\n",
    "\n",
    "for ngram in bigram_probas:\n",
    "    word_1, word_2 = ngram.split()\n",
    "    matrix[word2id[word_1]][word2id[word_2]] = bigram_probas[ngram] / unigram_probas[word_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25f7eb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мешочек'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[np.argmax(matrix[word2id['красный']])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "01fe5df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'заперла красный': 1,\n",
       " 'красный мешочек': 5,\n",
       " 'держа красный': 1,\n",
       " 'потом красный': 1,\n",
       " 'красный огонь': 1,\n",
       " ', красный': 1,\n",
       " 'красный ,': 2,\n",
       " 'лбом красный': 1,\n",
       " 'красный платок': 1,\n",
       " 'навстречу красный': 1,\n",
       " 'перекатывавший красный': 1,\n",
       " 'красный .': 1,\n",
       " 'маленький красный': 1,\n",
       " 'но красный': 1,\n",
       " 'откинула красный': 1}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "{el: value for el, value in bigrams_bag.items() if re.findall('\\\\bкрасный\\\\b', el)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677afdf3",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbce216",
   "metadata": {},
   "source": [
    "Расчитайте ненормализованное растояние Дамерау-Левенштейна и ненормализованное растояние Левенштейна между этими словами (каждое с каждым), найдите пару слов, для которых эти расстояния отличаются. Вставьте эти слова в форму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c37226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"решение\",\"ршеение\",\"ренешик\",\"рещиние\",\"ришение\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "79d248fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "582c5e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "решение решение 0 0 True\n",
      "решение ршеение 2 1 False\n",
      "решение ренешик 3 3 True\n",
      "решение рещиние 2 2 True\n",
      "решение ришение 1 1 True\n",
      "ршеение решение 2 1 False\n",
      "ршеение ршеение 0 0 True\n",
      "ршеение ренешик 4 4 True\n",
      "ршеение рещиние 3 3 True\n",
      "ршеение ришение 2 2 True\n",
      "ренешик решение 3 3 True\n",
      "ренешик ршеение 4 4 True\n",
      "ренешик ренешик 0 0 True\n",
      "ренешик рещиние 4 4 True\n",
      "ренешик ришение 4 4 True\n",
      "рещиние решение 2 2 True\n",
      "рещиние ршеение 3 3 True\n",
      "рещиние ренешик 4 4 True\n",
      "рещиние рещиние 0 0 True\n",
      "рещиние ришение 3 3 True\n",
      "ришение решение 1 1 True\n",
      "ришение ршеение 2 2 True\n",
      "ришение ренешик 4 4 True\n",
      "ришение рещиние 3 3 True\n",
      "ришение ришение 0 0 True\n"
     ]
    }
   ],
   "source": [
    "for word_1 in words:\n",
    "    for word_2 in words:\n",
    "        print(word_1, word_2, textdistance.levenshtein(word_1, word_2), textdistance.damerau_levenshtein(word_1, word_2), textdistance.levenshtein(word_1, word_2) == textdistance.damerau_levenshtein(word_1, word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7dff1d",
   "metadata": {},
   "source": [
    "## Задание 5\n",
    "\n",
    "Загрузите токенизатор distilbert-base-uncased из huggingface. Токенизируйте предложения из Анны Карениной с помощью предобученного токенизатора и обучите Fastext модель со следующими параметрами: cbow, размер вектора 300, размер окна 5, минимальный размер символьного нграмма - 2, максимальный размер символьного нграмма - 7. Найдите ближайшее слово к слову \"каренина\", вставьте его в форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9ad22ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 80 from C header, got 96 from PyObject\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a93b68ae6e440269bd10587e2b2c757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8a8a5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_bert_tokenizer(text):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    return [tokenizer.decode([x]) for x in encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fca48612",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'п',\n",
       " '##р',\n",
       " '##и',\n",
       " '##в',\n",
       " '##е',\n",
       " '##т',\n",
       " 'м',\n",
       " '##и',\n",
       " '##р',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_bert_tokenizer('Привет мир!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "151e57df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for sent in sentences:\n",
    "    corpus.append(decode_bert_tokenizer(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f3eeccb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]',\n",
       "  '«',\n",
       "  'а',\n",
       "  '##н',\n",
       "  '##на',\n",
       "  'к',\n",
       "  '##а',\n",
       "  '##р',\n",
       "  '##е',\n",
       "  '##н',\n",
       "  '##и',\n",
       "  '##на',\n",
       "  '»',\n",
       "  '–',\n",
       "  'э',\n",
       "  '##т',\n",
       "  '##о',\n",
       "  'с',\n",
       "  '##л',\n",
       "  '##о',\n",
       "  '##ж',\n",
       "  '##н',\n",
       "  '##о',\n",
       "  '##е',\n",
       "  ',',\n",
       "  'п',\n",
       "  '##с',\n",
       "  '##и',\n",
       "  '##х',\n",
       "  '##о',\n",
       "  '##л',\n",
       "  '##о',\n",
       "  '##г',\n",
       "  '##и',\n",
       "  '##ч',\n",
       "  '##е',\n",
       "  '##с',\n",
       "  '##к',\n",
       "  '##и',\n",
       "  'у',\n",
       "  '##т',\n",
       "  '##о',\n",
       "  '##н',\n",
       "  '##ч',\n",
       "  '##е',\n",
       "  '##н',\n",
       "  '##н',\n",
       "  '##о',\n",
       "  '##е',\n",
       "  ',',\n",
       "  'о',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##р',\n",
       "  '##о',\n",
       "  '##п',\n",
       "  '##р',\n",
       "  '##о',\n",
       "  '##б',\n",
       "  '##л',\n",
       "  '##е',\n",
       "  '##м',\n",
       "  '##н',\n",
       "  '##о',\n",
       "  '##е',\n",
       "  'п',\n",
       "  '##р',\n",
       "  '##о',\n",
       "  '##и',\n",
       "  '##з',\n",
       "  '##в',\n",
       "  '##е',\n",
       "  '##д',\n",
       "  '##е',\n",
       "  '##н',\n",
       "  '##и',\n",
       "  '##е',\n",
       "  ',',\n",
       "  'н',\n",
       "  '##а',\n",
       "  '##с',\n",
       "  '##ы',\n",
       "  '##щ',\n",
       "  '##е',\n",
       "  '##н',\n",
       "  '##н',\n",
       "  '##о',\n",
       "  '##е',\n",
       "  'п',\n",
       "  '##р',\n",
       "  '##и',\n",
       "  '##м',\n",
       "  '##е',\n",
       "  '##т',\n",
       "  '##а',\n",
       "  '##м',\n",
       "  '##и',\n",
       "  'в',\n",
       "  '##р',\n",
       "  '##е',\n",
       "  '##м',\n",
       "  '##е',\n",
       "  '##н',\n",
       "  '##и',\n",
       "  '.',\n",
       "  'л',\n",
       "  '.',\n",
       "  'н',\n",
       "  '.',\n",
       "  'т',\n",
       "  '##о',\n",
       "  '##л',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##о',\n",
       "  '##и',\n",
       "  'н',\n",
       "  '##а',\n",
       "  'с',\n",
       "  '##т',\n",
       "  '##р',\n",
       "  '##ан',\n",
       "  '##и',\n",
       "  '##ц',\n",
       "  '##а',\n",
       "  '##х',\n",
       "  'п',\n",
       "  '##р',\n",
       "  '##о',\n",
       "  '##и',\n",
       "  '##з',\n",
       "  '##в',\n",
       "  '##е',\n",
       "  '##д',\n",
       "  '##е',\n",
       "  '##н',\n",
       "  '##ия',\n",
       "  'п',\n",
       "  '##о',\n",
       "  '##ка',\n",
       "  '##з',\n",
       "  '##ы',\n",
       "  '##в',\n",
       "  '##а',\n",
       "  '##е',\n",
       "  '##т',\n",
       "  ',',\n",
       "  'к',\n",
       "  '##а',\n",
       "  '##к',\n",
       "  'р',\n",
       "  '##у',\n",
       "  '##ш',\n",
       "  '##а',\n",
       "  '##т',\n",
       "  '##с',\n",
       "  '##я',\n",
       "  'о',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##а',\n",
       "  '##т',\n",
       "  '##к',\n",
       "  '##и',\n",
       "  'п',\n",
       "  '##а',\n",
       "  '##т',\n",
       "  '##р',\n",
       "  '##и',\n",
       "  '##а',\n",
       "  '##р',\n",
       "  '##х',\n",
       "  '##а',\n",
       "  '##л',\n",
       "  '##ь',\n",
       "  '##н',\n",
       "  '##о',\n",
       "  '##г',\n",
       "  '##о',\n",
       "  'у',\n",
       "  '##к',\n",
       "  '##л',\n",
       "  '##а',\n",
       "  '##д',\n",
       "  '##а',\n",
       "  'ж',\n",
       "  '##и',\n",
       "  '##з',\n",
       "  '##н',\n",
       "  '##и',\n",
       "  'в',\n",
       "  'р',\n",
       "  '##о',\n",
       "  '##с',\n",
       "  '##с',\n",
       "  '##ии',\n",
       "  'п',\n",
       "  '##о',\n",
       "  '##д',\n",
       "  'н',\n",
       "  '##а',\n",
       "  '##т',\n",
       "  '##и',\n",
       "  '##с',\n",
       "  '##к',\n",
       "  '##о',\n",
       "  '##м',\n",
       "  'б',\n",
       "  '##у',\n",
       "  '##р',\n",
       "  '##ж',\n",
       "  '##у',\n",
       "  '##а',\n",
       "  '##з',\n",
       "  '##н',\n",
       "  '##о',\n",
       "  '##г',\n",
       "  '##о',\n",
       "  'п',\n",
       "  '##р',\n",
       "  '##о',\n",
       "  '##г',\n",
       "  '##р',\n",
       "  '##е',\n",
       "  '##с',\n",
       "  '##с',\n",
       "  '##а',\n",
       "  ',',\n",
       "  'к',\n",
       "  '##а',\n",
       "  '##к',\n",
       "  'п',\n",
       "  '##а',\n",
       "  '##д',\n",
       "  '##а',\n",
       "  '##ю',\n",
       "  '##т',\n",
       "  'н',\n",
       "  '##р',\n",
       "  '##а',\n",
       "  '##в',\n",
       "  '##ы',\n",
       "  ',',\n",
       "  'о',\n",
       "  '##с',\n",
       "  '##л',\n",
       "  '##а',\n",
       "  '##б',\n",
       "  '##е',\n",
       "  '##в',\n",
       "  '##а',\n",
       "  '##ю',\n",
       "  '##т',\n",
       "  'с',\n",
       "  '##е',\n",
       "  '##м',\n",
       "  '##е',\n",
       "  '##и',\n",
       "  '##н',\n",
       "  '##ы',\n",
       "  '##е',\n",
       "  'у',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##о',\n",
       "  '##и',\n",
       "  ',',\n",
       "  'в',\n",
       "  '##ы',\n",
       "  '##р',\n",
       "  '##о',\n",
       "  '##ж',\n",
       "  '##д',\n",
       "  '##а',\n",
       "  '##е',\n",
       "  '##т',\n",
       "  '##с',\n",
       "  '##я',\n",
       "  'а',\n",
       "  '##р',\n",
       "  '##и',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##о',\n",
       "  '##к',\n",
       "  '##р',\n",
       "  '##а',\n",
       "  '##т',\n",
       "  '##ия',\n",
       "  '.',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'р',\n",
       "  '##о',\n",
       "  '##м',\n",
       "  '##ан',\n",
       "  'в',\n",
       "  '##о',\n",
       "  'м',\n",
       "  '##н',\n",
       "  '##о',\n",
       "  '##г',\n",
       "  '##о',\n",
       "  '##м',\n",
       "  'а',\n",
       "  '##в',\n",
       "  '##т',\n",
       "  '##о',\n",
       "  '##б',\n",
       "  '##и',\n",
       "  '##о',\n",
       "  '##г',\n",
       "  '##р',\n",
       "  '##а',\n",
       "  '##ф',\n",
       "  '##и',\n",
       "  '##ч',\n",
       "  '##е',\n",
       "  '##н',\n",
       "  '.',\n",
       "  'р',\n",
       "  '##а',\n",
       "  '##б',\n",
       "  '##о',\n",
       "  '##т',\n",
       "  '##а',\n",
       "  '##я',\n",
       "  'н',\n",
       "  '##а',\n",
       "  '##д',\n",
       "  'н',\n",
       "  '##и',\n",
       "  '##м',\n",
       "  ',',\n",
       "  'т',\n",
       "  '##о',\n",
       "  '##л',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##о',\n",
       "  '##и',\n",
       "  'у',\n",
       "  '##я',\n",
       "  '##с',\n",
       "  '##н',\n",
       "  '##я',\n",
       "  '##л',\n",
       "  'в',\n",
       "  '##з',\n",
       "  '##г',\n",
       "  '##л',\n",
       "  '##я',\n",
       "  '##д',\n",
       "  'н',\n",
       "  '##а',\n",
       "  'с',\n",
       "  '##ов',\n",
       "  '##р',\n",
       "  '##е',\n",
       "  '##м',\n",
       "  '##е',\n",
       "  '##н',\n",
       "  '##н',\n",
       "  '##о',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##ь',\n",
       "  'и',\n",
       "  'с',\n",
       "  '##в',\n",
       "  '##о',\n",
       "  '##ю',\n",
       "  'с',\n",
       "  '##о',\n",
       "  '##б',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##в',\n",
       "  '##е',\n",
       "  '##н',\n",
       "  '##н',\n",
       "  '##у',\n",
       "  '##ю',\n",
       "  'ж',\n",
       "  '##и',\n",
       "  '##з',\n",
       "  '##н',\n",
       "  '##ь',\n",
       "  '.',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'л',\n",
       "  '##е',\n",
       "  '##в',\n",
       "  'н',\n",
       "  '##и',\n",
       "  '##к',\n",
       "  '##о',\n",
       "  '##л',\n",
       "  '##а',\n",
       "  '##евич',\n",
       "  'т',\n",
       "  '##о',\n",
       "  '##л',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##о',\n",
       "  '##и',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'ч',\n",
       "  '##а',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##ь',\n",
       "  'п',\n",
       "  '##е',\n",
       "  '##р',\n",
       "  '##в',\n",
       "  '##а',\n",
       "  '##я',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'ч',\n",
       "  '##а',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##ь',\n",
       "  'в',\n",
       "  '##т',\n",
       "  '##о',\n",
       "  '##р',\n",
       "  '##а',\n",
       "  '##я',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'ч',\n",
       "  '##а',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##ь',\n",
       "  'т',\n",
       "  '##р',\n",
       "  '##е',\n",
       "  '##т',\n",
       "  '##ь',\n",
       "  '##я',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'ч',\n",
       "  '##а',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##ь',\n",
       "  'ч',\n",
       "  '##е',\n",
       "  '##т',\n",
       "  '##в',\n",
       "  '##е',\n",
       "  '##р',\n",
       "  '##т',\n",
       "  '##а',\n",
       "  '##я',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'ч',\n",
       "  '##а',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##ь',\n",
       "  'п',\n",
       "  '##я',\n",
       "  '##т',\n",
       "  '##а',\n",
       "  '##я',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'ч',\n",
       "  '##а',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##ь',\n",
       "  'ш',\n",
       "  '##е',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##а',\n",
       "  '##я',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'ч',\n",
       "  '##а',\n",
       "  '##с',\n",
       "  '##т',\n",
       "  '##ь',\n",
       "  'с',\n",
       "  '##е',\n",
       "  '##д',\n",
       "  '##ь',\n",
       "  '##м',\n",
       "  '##а',\n",
       "  '##я',\n",
       "  '[SEP]']]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "797a59bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4052400, 13888640)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1\n",
    "        \n",
    "epoch_logger = EpochLogger()\n",
    "\n",
    "model = FastText(vector_size=300, window=5, sg=0, min_n=2, max_n=7)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_examples=len(corpus), epochs=10, callbacks=[epoch_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "683ba37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('##ка', 0.5696330070495605),\n",
       " ('а', 0.49767211079597473),\n",
       " ('##на', 0.40630394220352173),\n",
       " ('##ович', 0.36692699790000916),\n",
       " ('–', 0.2744726538658142),\n",
       " ('к', 0.2697364389896393),\n",
       " ('ф', 0.2500544488430023),\n",
       " ('с', 0.20072151720523834),\n",
       " ('з', 0.1963053047657013),\n",
       " ('[SEP]', 0.18371255695819855)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('каренина')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5a1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
