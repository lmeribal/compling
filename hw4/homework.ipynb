{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "news = open('lenta.txt').read()\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab_news = Counter(norm_news)\n",
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)][:-100]\n",
    "sentence_perp = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)][-100:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "threegrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    threegrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = np.zeros((len(bigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "\n",
    "id2word_news_uni = list(unigrams_news)\n",
    "word2id_news_uni = {word:i for i, word in enumerate(id2word_news_uni)}\n",
    "\n",
    "id2word_news_bi = list(bigrams_news)\n",
    "word2id_news_bi = {word:i for i, word in enumerate(id2word_news_bi)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in threegrams_news:\n",
    "    word1 = \" \".join(ngram.split()[:2])\n",
    "    word2 = ngram.split()[-1]\n",
    "    matrix_news[word2id_news_bi[word1]][word2id_news_uni[word2]] =  (threegrams_news[ngram]/\n",
    "                                                                     bigrams_news[word1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word_bi, id2word_uni, word2id_bi, n=200, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = word2id_bi[start]\n",
    "    for i in range(n):\n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        text.append(id2word_uni[chosen])\n",
    "        if id2word_uni[chosen] == '<end>':\n",
    "            current_idx = word2id_bi[start]\n",
    "        else:\n",
    "            part = id2word_bi[current_idx] + ' ' + id2word_uni[chosen]\n",
    "            part = ' '.join(part.split()[1:])\n",
    "            current_idx = word2id_bi[part]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "гуманное изобретение сохранило тысячи жизней на сибирских этапах и пересылках \n",
      "\n",
      " ранее предполагалось что именно наумов вместе с женой набирали код подъезда \n",
      "\n",
      " по его оценке перспективы возможных переговоров с кредиторами о реструктуризации российских долговчастным инвесторам и кредиторам \n",
      "\n",
      " этот ответ должен последовать рост рекламных расценок на каналах для обеспечения правопорядка а для назначения в администрацию президента путина накануне сегодняшнего заседания этот вопрос был ли это уголовное дело в том что некоторые документы для регистрации требуется указать серийный номер программы и интернет-приложения \n",
      "\n",
      " c ледствие не исключает возможности расследования связанного с рассмотрением заявлений производится по плану предложенному бараком аната должна была пройти военная автоколонна и неподалеку от подмосковного солнечногорска столкнулись не менее представители microsoft уже опубликовала первую заплатку для internet explorer и скрипт-машиной которая непосредственно выполняет скрипт-программу например microsoft не присутствовал но присутствовал президент ао газ николай пугин заявил что курс рубля будет достаточно для того чтобы соответствовать пожеланиям правительства \n",
      "\n",
      " никто из рабочих предприятия автодор прокладывавших трассу между селениями нижняя чемульга а также планы по рождению ребенка они вынашивали в течение полутора лет после свержения наполеона iii 50-летний парижанин шарль наполеон праправнук жерома наполеона брата императора наполеона i бонапарта выдвинул свою кандидатуру в пользу твц о том что доходы\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news_bi, id2word_news_uni, word2id_news_bi).replace('<end>', '\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> <start> как сообщили ведомости глава кемеровской области аман тулеев уже выпустил постановление рекомендующее предприятиям региона рассмотреть возможность сотрудничества с глобальной русской информационной сетью grin ru <end> 21677.33893298278 \n",
      "\n",
      "<start> <start> эта сеть была создана в начале года бывшим президентом московской фондовой биржи виктором сахаровым совместно с бывшим министром иностранных дел андреем козыревым и старается объединить все сектора экономики от сельского хозяйства до металлургии являясь таким образом многоотраслевым порталом b 2 b участниками которого стали уже 300 предприятий <end> 16496.45753273551 \n",
      "\n",
      "<start> <start> в отличие от других видов электронной коммерции в 2 в привлекателен тем что денежные обороты здесь более значительны <end> 9101.96870185863 \n",
      "\n",
      "<start> <start> так по данным агентства infoart news за 10 дней со дня открытия интернет-биржи зерно он-лайн где происходит торговля зерном сахаром подсолнечником и другими унифицированными сырьевыми товарами в россии казахстане и на украине было заключено сделок на сумму свыше 200 тысяч долларов в отчете выпущенном в пятницу счетная палата конгресса сша высказала мнение что администрация клинтона на законных основаниях передала в 1998 году полномочия по регистрации доменных имен компании internet corporation for assigned names and numbers icann <end> 2303.5450798308466 \n",
      "\n",
      "<start> <start> счетная палата заявила также что icann может устанавливать плату за регистрацию доменов по своему усмотрению при условии что цены эти будут приемлемыми <end> 6003.55477010265 \n",
      "\n",
      "<start> <start> в прошлом году icann предложила принять размер годовой платы для владельцев доменов равным 1 доллару сша <end> 20319.517130890308 \n",
      "\n",
      "<start> <start> несмотря на легальность деятельности icann представителями счетной палаты было высказано заявление что передача государственного контроля над корневым dns-сервером осуществленная департаментом коммерции может быть сочтена незаконной <end> 27544.71893096978 \n",
      "\n",
      "<start> <start> в отчете занявшем 45 страниц сообщается неясно обладает ли департамент требуемыми полномочиями чтобы передать контроль над корневым сервером <end> 47189.28432652307 \n",
      "\n",
      "<start> <start> если действия департамента коммерции будут сочтены незаконными и icann не будет иметь доступа к серверу то создание новых доменов xxx biz или web будет зависеть от правительства сша <end> 11725.60642231721 \n",
      "\n",
      "<start> <start> в мае icann предприняла первые шаги по регистрации новых доменов <end> 27546.974560994437 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    return p ** (-1/N) \n",
    "\n",
    "for sent in sentence_perp[:10]:\n",
    "    prob = []\n",
    "    for ngram in ngrammer(sent, n=3):\n",
    "        word1 = \" \".join(ngram.split()[:2])\n",
    "        word2 = ngram.split()[-1]\n",
    "        if ngram in threegrams_news and word1 in bigrams_news:\n",
    "            prob.append(np.log(threegrams_news[ngram] / bigrams_news[word1]))\n",
    "        else:\n",
    "            prob.append(np.log(0.00001))\n",
    "    print(\" \".join(sent), perplexity(prob), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во-первых, мы можем выбрать фиксированный словарь токенов, и ввести новый токен \\<UNKN\\>, который будем приписывать на этапе нормализации всем словам, которые не входят в словарь. Затем можем считать вероятность этого токена \\<UNKN\\> как любого другого\n",
    "\n",
    "Во-вторых, мы можем создать \"скрытый\" словарь, приписывая токен \\<UNKN\\> всем словам, которые не проходят частотный порог, а затем снова обучить языковую модель, рассматривая \\<UNKN\\> как обычное слово"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
